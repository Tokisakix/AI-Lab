# 使用决策树完成信誉度分类任务

## 文件结构

* `DecisionTree.ipynb`：代码。

  **准确率: 84.29%**

* `Task.pdf`: 任务描述。

* `Tutor.pptx`: 实验课件。

* `data/`：所需要的数据。

## 算法原理

### 决策树算法

决策树是最经典的机器学习算法之一，主要用于分类和回归任务。其构建过程基于递归分割：

- 从根节点开始，选择一个最优特征进行数据分割，生成子节点；

- 在每个子节点上重复此过程，直到满足停止条件，如节点纯度达到一定程度、达到预设的最大深度等。

### ## 特征划分方法

上面说过，决策树就是不断分割特征的过程。这里介绍两种常见的分割算法。

### ID3算法

ID3 (Iterative Dichotomiser 3) 算法使用信息增益作为划分特征的标准。

给定训练集 D 和特征 A，信息增益 $\text{Gain}(D, A)$ 定义为：

$$
\text{Gain}(D, A) = \text{Entropy}(D) - \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} \text{Entropy}(D_v)
$$

其中，$\text{Entropy}(D)$ 是数据集 D 的熵，计算公式为：

$$
\text{Entropy}(D) = -\sum_{k} p_k \log_2 p_k
$$

$p_k$ 是类别 k 在数据集 D 中的比例。

### C4.5算法

C4.5 算法改进了 ID3，通过使用信息增益率来克服 ID3 偏好选择具有更多值的特征的问题。

信息增益率定义为信息增益和特征 A 的固有信息（分割信息）之比：

$$
\text{GainRatio}(D, A) = \frac{\text{Gain}(D, A)}{\text{SplitInfo}(D, A)}
$$

其中，分割信息 $\text{SplitInfo}(D, A)$ 的计算公式为：

$$
\text{SplitInfo}(D, A) = -\sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} \log_2 \frac{|D_v|}{|D|}
$$

### 剪枝方法

剪枝是决策树算法中用于减少过拟合和提高模型泛化能力的重要技术。

### 预剪枝

预剪枝在决策树完全生成前进行。预剪枝提前停止某些分支的生长，防止过拟合。这可以通过设定最大深度、最小样本数或最小信息增益等参数来实现。

### 后剪枝

后剪枝在决策树完全生成后进行。后剪枝从树的底部开始，尝试去除某些子节点（用叶节点替换），并检验这种简化是否能提高测试集上的准确率，从而优化决策树的结构。